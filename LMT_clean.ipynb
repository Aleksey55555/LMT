{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aleksey55555/LMT/blob/master/LMT_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install optuna"
      ],
      "metadata": {
        "id": "FKmpMBz6Jtxw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VDbhd-pJkCr"
      },
      "execution_count": 5,
      "outputs": [],
      "source": [
        "from math import ceil\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, fbeta_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, fbeta_score, accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score, fbeta_score, accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import numpy as np\n",
        "import optuna\n",
        "import optuna.visualization as viz\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Идея создать такой классификатор LMT (logistic model tree), который будет сочитать дерево решений и логистическую регрессию. В каждом листе будет логистическая регрессия на признаках, которые не использовались в ветвлении дерева.\n",
        "Подход\n",
        "\n",
        "Строим дерево по подмножеству признаков:\n",
        "\n",
        "на каждом узле выбираем признак и порог для разбиения (как в DecisionTreeClassifier).\n",
        "\n",
        "глубина/мин-сэмплы ограничивают переобучение.\n",
        "\n",
        "В листьях:\n",
        "\n",
        "берём только те признаки, которые не использовались для делений выше по пути.\n",
        "\n",
        "обучаем LogisticRegression на этом подмножестве данных.\n",
        "\n",
        "Предсказание:\n",
        "\n",
        "объект проходит по дереву до листа.\n",
        "\n",
        "в листе к нему применяется локальная логистическая регрессия.\n",
        "\n",
        "Реализация с помощью scikit-learn"
      ],
      "metadata": {
        "id": "xX4G9EZ1wzm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticModelTree(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, max_depth=3, min_samples_leaf=20, random_state=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.random_state = random_state\n",
        "    def fit(self, X, y):\n",
        "        # шаг 1: строим дерево только для разбиений\n",
        "        self.tree_ = DecisionTreeClassifier(\n",
        "            max_depth=self.max_depth,\n",
        "            min_samples_leaf=self.min_samples_leaf,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        self.tree_.fit(X, y)\n",
        "        # шаг 2: находим индексы объектов в листьях\n",
        "        leaf_ids = self.tree_.apply(X)\n",
        "        self.models_ = {}\n",
        "        self.classes_ = self.tree_.classes_\n",
        "        for leaf in np.unique(leaf_ids):\n",
        "            mask = (leaf_ids == leaf)\n",
        "            # получаем признаки, использованные на пути до этого листа\n",
        "            path_features = self._get_features_on_path(leaf)\n",
        "            remaining_features = [i for i in range(X.shape[1]) if i not in path_features]\n",
        "            if not remaining_features:\n",
        "                remaining_features = list(range(X.shape[1]))\n",
        "            X_leaf = X[mask][:, remaining_features]\n",
        "            y_leaf = y[mask]\n",
        "            if len(np.unique(y_leaf)) == 1:\n",
        "                # \"чистый\" лист: всегда один класс\n",
        "                class_idx = np.where(self.classes_ == y_leaf[0])[0][0]\n",
        "                def dummy_model(X_input, c=class_idx):\n",
        "                    proba = np.zeros((X_input.shape[0], len(self.classes_)))\n",
        "                    proba[:, c] = 1.0\n",
        "                    return proba\n",
        "                self.models_[leaf] = (dummy_model, remaining_features, True)\n",
        "            else:\n",
        "                model = LogisticRegression(max_iter=500)\n",
        "                model.fit(X_leaf, y_leaf)\n",
        "                self.models_[leaf] = (model, remaining_features, False)\n",
        "        return self\n",
        "    def predict_proba(self, X):\n",
        "        leaf_ids = self.tree_.apply(X)\n",
        "        proba = np.zeros((X.shape[0], len(self.classes_)))\n",
        "        for leaf, (model, feats, is_dummy) in self.models_.items():\n",
        "            mask = (leaf_ids == leaf)\n",
        "            if np.any(mask):\n",
        "                X_leaf = X[mask][:, feats]\n",
        "                if is_dummy:\n",
        "                    proba[mask] = model(X_leaf)\n",
        "                else:\n",
        "                    proba[mask] = model.predict_proba(X_leaf)\n",
        "        return proba\n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.predict_proba(X), axis=1)\n",
        "    def _get_features_on_path(self, leaf_id):\n",
        "        \"\"\"Собрать все признаки, использованные на пути до данного листа\"\"\"\n",
        "        tree = self.tree_.tree_\n",
        "        path_features = set()\n",
        "        def recurse(node, path):\n",
        "            if node == leaf_id:\n",
        "                return path\n",
        "            if tree.feature[node] >= 0:\n",
        "                left = tree.children_left[node]\n",
        "                right = tree.children_right[node]\n",
        "                if left != -1:\n",
        "                    res = recurse(left, path | {tree.feature[node]})\n",
        "                    if res is not None:\n",
        "                        return res\n",
        "                if right != -1:\n",
        "                    res = recurse(right, path | {tree.feature[node]})\n",
        "                    if res is not None:\n",
        "                        return res\n",
        "            return None\n",
        "        return recurse(0, set()) or set()\n"
      ],
      "metadata": {
        "id": "QIkVWGZ79Ehc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим метрики на датасете breast_cancer"
      ],
      "metadata": {
        "id": "_zW4i7j1zNgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "clf = LogisticModelTree(max_depth=3, min_samples_leaf=30, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, clf.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2pHVbpC9Gt-",
        "outputId": "e7a36e85-b87f-4c98-e41c-ad4a81441746"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, clf.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evl7SiL29aha",
        "outputId": "b7636029-d7b4-44a1-f4f0-6356df6fc3cb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96        63\n",
            "           1       0.96      0.99      0.98       108\n",
            "\n",
            "    accuracy                           0.97       171\n",
            "   macro avg       0.97      0.96      0.97       171\n",
            "weighted avg       0.97      0.97      0.97       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Точность (precision)\n",
        "\n",
        "Класс 0: 0.98\n",
        "\n",
        "Класс 1: 0.96\n",
        "→ почти без ложноположительных ошибок.\n",
        "\n",
        "Полнота (recall)\n",
        "\n",
        "Класс 0: 0.94\n",
        "\n",
        "Класс 1: 0.99\n",
        "→ модель чуть чаще путает класс 0\n",
        "\n",
        "F1-score\n",
        "\n",
        "Оба класса ≈ 0.96–0.98 → очень сбалансировано.\n",
        "\n",
        "\n",
        " Сравним  с другими моделями: RandomForestClassifier, LogisticRegression, XGBClassifier."
      ],
      "metadata": {
        "id": "_pWFY5Gz0J-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# обучаем все модели\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
        "    \"XGBoost\": XGBClassifier(\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=4,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        eval_metric=\"logloss\",\n",
        "        use_label_encoder=False,\n",
        "        random_state=42\n",
        "    ),\n",
        "    \"Logistic Model Tree\": LogisticModelTree(max_depth=3, min_samples_leaf=30, random_state=42)\n",
        "}\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\n{name}\")\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    results[name] = acc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPNeEUwGzr67",
        "outputId": "911a7949-8102-41d4-9967-506c2cb517ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Logistic Regression\n",
            "Accuracy: 0.9707602339181286\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.95      0.96        63\n",
            "           1       0.97      0.98      0.98       108\n",
            "\n",
            "    accuracy                           0.97       171\n",
            "   macro avg       0.97      0.97      0.97       171\n",
            "weighted avg       0.97      0.97      0.97       171\n",
            "\n",
            "\n",
            "Random Forest\n",
            "Accuracy: 0.9707602339181286\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96        63\n",
            "           1       0.96      0.99      0.98       108\n",
            "\n",
            "    accuracy                           0.97       171\n",
            "   macro avg       0.97      0.96      0.97       171\n",
            "weighted avg       0.97      0.97      0.97       171\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [13:27:13] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "XGBoost\n",
            "Accuracy: 0.9590643274853801\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.94      0.94        63\n",
            "           1       0.96      0.97      0.97       108\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.96      0.95      0.96       171\n",
            "weighted avg       0.96      0.96      0.96       171\n",
            "\n",
            "\n",
            "Logistic Model Tree\n",
            "Accuracy: 0.9707602339181286\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96        63\n",
            "           1       0.96      0.99      0.98       108\n",
            "\n",
            "    accuracy                           0.97       171\n",
            "   macro avg       0.97      0.96      0.97       171\n",
            "weighted avg       0.97      0.97      0.97       171\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LMT и Random Forest показали одинаковый результат, LogReg с таким же accuracy 0,971, но recall чуть хуже (на классе 1, что важно для данного набора). XGBoost дал хуже результат  accuract - 0.959"
      ],
      "metadata": {
        "id": "rWGkDhl7zy_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем улучшить модель LMT, добавив возможность использования небольшого количества признаков, использованнных для ветвления в логистической регрессии в листе. Гипрепараметр reuse_ratio=0.1"
      ],
      "metadata": {
        "id": "J-Hk-3fW7EcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticModelTree(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, max_depth=3, min_samples_leaf=20, random_state=None, reuse_ratio=0.1):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.random_state = random_state\n",
        "        self.reuse_ratio = reuse_ratio  # доля признаков из пути, которые можно \"вернуть\"\n",
        "    def fit(self, X, y):\n",
        "        # шаг 1: строим дерево для разбиений\n",
        "        self.tree_ = DecisionTreeClassifier(\n",
        "            max_depth=self.max_depth,\n",
        "            min_samples_leaf=self.min_samples_leaf,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        self.tree_.fit(X, y)\n",
        "        # шаг 2: распределяем объекты по листьям\n",
        "        leaf_ids = self.tree_.apply(X)\n",
        "        self.models_ = {}\n",
        "        self.classes_ = self.tree_.classes_\n",
        "        self.leaf_samples_ = {}\n",
        "        rng = np.random.RandomState(self.random_state)\n",
        "        for leaf in np.unique(leaf_ids):\n",
        "            mask = (leaf_ids == leaf)\n",
        "            self.leaf_samples_[leaf] = np.sum(mask)\n",
        "            # признаки, использованные на пути\n",
        "            path_features = list(self._get_features_on_path(leaf))\n",
        "            unused_features = [i for i in range(X.shape[1]) if i not in path_features]\n",
        "            # пропорция признаков из пути\n",
        "            k = max(1, int(len(path_features) * self.reuse_ratio)) if path_features else 0\n",
        "            reuse_features = rng.choice(path_features, size=k, replace=False).tolist() if k > 0 else []\n",
        "            final_features = unused_features + reuse_features\n",
        "            if not final_features:  # fallback\n",
        "                final_features = list(range(X.shape[1]))\n",
        "            X_leaf = X[mask][:, final_features]\n",
        "            y_leaf = y[mask]\n",
        "            if len(np.unique(y_leaf)) == 1:\n",
        "                # чистый лист\n",
        "                class_idx = np.where(self.classes_ == y_leaf[0])[0][0]\n",
        "                def dummy_model(X_input, c=class_idx):\n",
        "                    proba = np.zeros((X_input.shape[0], len(self.classes_)))\n",
        "                    proba[:, c] = 1.0\n",
        "                    return proba\n",
        "                self.models_[leaf] = (dummy_model, final_features, True, class_idx, None)\n",
        "            else:\n",
        "                model = LogisticRegression(max_iter=500)\n",
        "                model.fit(X_leaf, y_leaf)\n",
        "                self.models_[leaf] = (model, final_features, False, None, model.coef_)\n",
        "        return self\n",
        "    def predict_proba(self, X):\n",
        "        leaf_ids = self.tree_.apply(X)\n",
        "        proba = np.zeros((X.shape[0], len(self.classes_)))\n",
        "        for leaf, (model, feats, is_dummy, _, _) in self.models_.items():\n",
        "            mask = (leaf_ids == leaf)\n",
        "            if np.any(mask):\n",
        "                X_leaf = X[mask][:, feats]\n",
        "                if is_dummy:\n",
        "                    proba[mask] = model(X_leaf)\n",
        "                else:\n",
        "                    proba[mask] = model.predict_proba(X_leaf)\n",
        "        return proba\n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.predict_proba(X), axis=1)\n",
        "    def _get_features_on_path(self, leaf_id):\n",
        "        \"\"\"Собрать все признаки, использованные на пути до данного листа\"\"\"\n",
        "        tree = self.tree_.tree_\n",
        "        path_features = set()\n",
        "        def recurse(node, path):\n",
        "            if tree.children_left[node] == -1 and tree.children_right[node] == -1:\n",
        "                if node == leaf_id:\n",
        "                    return path\n",
        "                return None\n",
        "            if tree.feature[node] >= 0:\n",
        "                left = tree.children_left[node]\n",
        "                right = tree.children_right[node]\n",
        "                if left != -1:\n",
        "                    res = recurse(left, path | {tree.feature[node]})\n",
        "                    if res is not None:\n",
        "                        return res\n",
        "                if right != -1:\n",
        "                    res = recurse(right, path | {tree.feature[node]})\n",
        "                    if res is not None:\n",
        "                        return res\n",
        "            return None\n",
        "        return recurse(0, set()) or set()\n",
        "    def print_leaf_stats(self, feature_names=None):\n",
        "        \"\"\"Вывести статистику по каждому листу\"\"\"\n",
        "        for leaf, (model, feats, is_dummy, class_idx, coefs) in self.models_.items():\n",
        "            print(\"=\"*60)\n",
        "            print(f\"Лист {leaf} | объектов: {self.leaf_samples_[leaf]}\")\n",
        "            used_feats = self._get_features_on_path(leaf)\n",
        "            if feature_names is not None:\n",
        "                used_feats = [feature_names[i] for i in used_feats]\n",
        "                feats_names = [feature_names[i] for i in feats]\n",
        "            else:\n",
        "                feats_names = feats\n",
        "            print(f\"  Использованные признаки на пути: {used_feats}\")\n",
        "            print(f\"  Признаки в логрег: {feats_names}\")\n",
        "            if is_dummy:\n",
        "                print(f\"  Модель: ЧИСТЫЙ ЛИСТ → всегда класс {self.classes_[class_idx]}\")\n",
        "            else:\n",
        "                print(\"  Модель: Логистическая регрессия\")\n",
        "                print(\"   Коэффициенты:\")\n",
        "                for i, c in enumerate(coefs[0]):\n",
        "                    fname = feats_names[i]\n",
        "                    print(f\"     {fname}: {c:.4f}\")\n"
      ],
      "metadata": {
        "id": "VLXip4mwzzbs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticModelTree(max_depth=3, min_samples_leaf=30, random_state=42, reuse_ratio=0.2)\n",
        "clf.fit(X_train, y_train)\n",
        "clf.print_leaf_stats(feature_names=load_breast_cancer().feature_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZIK3l-Mz6fh",
        "outputId": "caa531ac-70bf-41fb-9c45-56c5be51fa14"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Лист 3 | объектов: 148\n",
            "  Использованные признаки на пути: [np.str_('texture error'), np.str_('worst area'), np.str_('mean concave points')]\n",
            "  Признаки в логрег: [np.str_('mean radius'), np.str_('mean texture'), np.str_('mean perimeter'), np.str_('mean area'), np.str_('mean smoothness'), np.str_('mean compactness'), np.str_('mean concavity'), np.str_('mean symmetry'), np.str_('mean fractal dimension'), np.str_('radius error'), np.str_('perimeter error'), np.str_('area error'), np.str_('smoothness error'), np.str_('compactness error'), np.str_('concavity error'), np.str_('concave points error'), np.str_('symmetry error'), np.str_('fractal dimension error'), np.str_('worst radius'), np.str_('worst texture'), np.str_('worst perimeter'), np.str_('worst smoothness'), np.str_('worst compactness'), np.str_('worst concavity'), np.str_('worst concave points'), np.str_('worst symmetry'), np.str_('worst fractal dimension'), np.str_('texture error')]\n",
            "  Модель: ЧИСТЫЙ ЛИСТ → всегда класс 1\n",
            "============================================================\n",
            "Лист 4 | объектов: 63\n",
            "  Использованные признаки на пути: [np.str_('texture error'), np.str_('worst area'), np.str_('mean concave points')]\n",
            "  Признаки в логрег: [np.str_('mean radius'), np.str_('mean texture'), np.str_('mean perimeter'), np.str_('mean area'), np.str_('mean smoothness'), np.str_('mean compactness'), np.str_('mean concavity'), np.str_('mean symmetry'), np.str_('mean fractal dimension'), np.str_('radius error'), np.str_('perimeter error'), np.str_('area error'), np.str_('smoothness error'), np.str_('compactness error'), np.str_('concavity error'), np.str_('concave points error'), np.str_('symmetry error'), np.str_('fractal dimension error'), np.str_('worst radius'), np.str_('worst texture'), np.str_('worst perimeter'), np.str_('worst smoothness'), np.str_('worst compactness'), np.str_('worst concavity'), np.str_('worst concave points'), np.str_('worst symmetry'), np.str_('worst fractal dimension'), np.str_('worst area')]\n",
            "  Модель: Логистическая регрессия\n",
            "   Коэффициенты:\n",
            "     mean radius: -0.0612\n",
            "     mean texture: 0.3351\n",
            "     mean perimeter: 0.0948\n",
            "     mean area: 0.0408\n",
            "     mean smoothness: -0.0155\n",
            "     mean compactness: 0.0294\n",
            "     mean concavity: -0.0176\n",
            "     mean symmetry: 0.0199\n",
            "     mean fractal dimension: -0.0054\n",
            "     radius error: 0.0182\n",
            "     perimeter error: -0.0315\n",
            "     area error: -0.1654\n",
            "     smoothness error: -0.0010\n",
            "     compactness error: 0.0293\n",
            "     concavity error: 0.0065\n",
            "     concave points error: -0.0015\n",
            "     symmetry error: 0.0172\n",
            "     fractal dimension error: 0.0036\n",
            "     worst radius: 0.0080\n",
            "     worst texture: -0.2385\n",
            "     worst perimeter: 0.7331\n",
            "     worst smoothness: -0.0441\n",
            "     worst compactness: 0.0640\n",
            "     worst concavity: -0.1385\n",
            "     worst concave points: -0.0411\n",
            "     worst symmetry: 0.0329\n",
            "     worst fractal dimension: -0.0057\n",
            "     worst area: -0.1056\n",
            "============================================================\n",
            "Лист 5 | объектов: 34\n",
            "  Использованные признаки на пути: [np.str_('worst area'), np.str_('mean concave points')]\n",
            "  Признаки в логрег: [np.str_('mean radius'), np.str_('mean texture'), np.str_('mean perimeter'), np.str_('mean area'), np.str_('mean smoothness'), np.str_('mean compactness'), np.str_('mean concavity'), np.str_('mean symmetry'), np.str_('mean fractal dimension'), np.str_('radius error'), np.str_('texture error'), np.str_('perimeter error'), np.str_('area error'), np.str_('smoothness error'), np.str_('compactness error'), np.str_('concavity error'), np.str_('concave points error'), np.str_('symmetry error'), np.str_('fractal dimension error'), np.str_('worst radius'), np.str_('worst texture'), np.str_('worst perimeter'), np.str_('worst smoothness'), np.str_('worst compactness'), np.str_('worst concavity'), np.str_('worst concave points'), np.str_('worst symmetry'), np.str_('worst fractal dimension'), np.str_('mean concave points')]\n",
            "  Модель: Логистическая регрессия\n",
            "   Коэффициенты:\n",
            "     mean radius: 0.0633\n",
            "     mean texture: 0.2842\n",
            "     mean perimeter: 0.8160\n",
            "     mean area: -0.0225\n",
            "     mean smoothness: -0.0065\n",
            "     mean compactness: 0.0142\n",
            "     mean concavity: -0.0036\n",
            "     mean symmetry: -0.0019\n",
            "     mean fractal dimension: -0.0002\n",
            "     radius error: 0.0307\n",
            "     texture error: 0.4170\n",
            "     perimeter error: 0.7607\n",
            "     area error: 0.0515\n",
            "     smoothness error: 0.0001\n",
            "     compactness error: 0.0187\n",
            "     concavity error: 0.0088\n",
            "     concave points error: 0.0046\n",
            "     symmetry error: 0.0045\n",
            "     fractal dimension error: 0.0019\n",
            "     worst radius: -0.8564\n",
            "     worst texture: -0.4965\n",
            "     worst perimeter: -0.5754\n",
            "     worst smoothness: -0.0278\n",
            "     worst compactness: 0.0363\n",
            "     worst concavity: -0.0317\n",
            "     worst concave points: -0.0056\n",
            "     worst symmetry: -0.0429\n",
            "     worst fractal dimension: -0.0010\n",
            "     mean concave points: -0.0057\n",
            "============================================================\n",
            "Лист 7 | объектов: 39\n",
            "  Использованные признаки на пути: [np.str_('worst perimeter'), np.str_('mean concave points')]\n",
            "  Признаки в логрег: [np.str_('mean radius'), np.str_('mean texture'), np.str_('mean perimeter'), np.str_('mean area'), np.str_('mean smoothness'), np.str_('mean compactness'), np.str_('mean concavity'), np.str_('mean symmetry'), np.str_('mean fractal dimension'), np.str_('radius error'), np.str_('texture error'), np.str_('perimeter error'), np.str_('area error'), np.str_('smoothness error'), np.str_('compactness error'), np.str_('concavity error'), np.str_('concave points error'), np.str_('symmetry error'), np.str_('fractal dimension error'), np.str_('worst radius'), np.str_('worst texture'), np.str_('worst area'), np.str_('worst smoothness'), np.str_('worst compactness'), np.str_('worst concavity'), np.str_('worst concave points'), np.str_('worst symmetry'), np.str_('worst fractal dimension'), np.str_('worst perimeter')]\n",
            "  Модель: Логистическая регрессия\n",
            "   Коэффициенты:\n",
            "     mean radius: 0.0904\n",
            "     mean texture: -0.5803\n",
            "     mean perimeter: 0.1409\n",
            "     mean area: 0.0526\n",
            "     mean smoothness: 0.0021\n",
            "     mean compactness: -0.0059\n",
            "     mean concavity: -0.0330\n",
            "     mean symmetry: -0.0202\n",
            "     mean fractal dimension: -0.0009\n",
            "     radius error: 0.0072\n",
            "     texture error: -0.0064\n",
            "     perimeter error: 0.0960\n",
            "     area error: 0.4161\n",
            "     smoothness error: -0.0003\n",
            "     compactness error: -0.0048\n",
            "     concavity error: -0.0084\n",
            "     concave points error: -0.0015\n",
            "     symmetry error: -0.0097\n",
            "     fractal dimension error: -0.0003\n",
            "     worst radius: 0.0392\n",
            "     worst texture: -1.0450\n",
            "     worst area: -0.0858\n",
            "     worst smoothness: -0.0014\n",
            "     worst compactness: -0.0503\n",
            "     worst concavity: -0.0923\n",
            "     worst concave points: -0.0221\n",
            "     worst symmetry: -0.0786\n",
            "     worst fractal dimension: -0.0059\n",
            "     worst perimeter: 0.0738\n",
            "============================================================\n",
            "Лист 8 | объектов: 114\n",
            "  Использованные признаки на пути: [np.str_('worst perimeter'), np.str_('mean concave points')]\n",
            "  Признаки в логрег: [np.str_('mean radius'), np.str_('mean texture'), np.str_('mean perimeter'), np.str_('mean area'), np.str_('mean smoothness'), np.str_('mean compactness'), np.str_('mean concavity'), np.str_('mean symmetry'), np.str_('mean fractal dimension'), np.str_('radius error'), np.str_('texture error'), np.str_('perimeter error'), np.str_('area error'), np.str_('smoothness error'), np.str_('compactness error'), np.str_('concavity error'), np.str_('concave points error'), np.str_('symmetry error'), np.str_('fractal dimension error'), np.str_('worst radius'), np.str_('worst texture'), np.str_('worst area'), np.str_('worst smoothness'), np.str_('worst compactness'), np.str_('worst concavity'), np.str_('worst concave points'), np.str_('worst symmetry'), np.str_('worst fractal dimension'), np.str_('mean concave points')]\n",
            "  Модель: ЧИСТЫЙ ЛИСТ → всегда класс 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Протетстируем переиспользование признаков при разном reuse_ratio"
      ],
      "metadata": {
        "id": "FQi5kSaF8D5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перепишем модель"
      ],
      "metadata": {
        "id": "doD-dTOE9vJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticModelTree(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, max_depth=3, min_samples_leaf=20, random_state=None,\n",
        "                 reuse_ratio=0.1, max_iter=5000, solver=\"lbfgs\"):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.random_state = random_state\n",
        "        self.reuse_ratio = reuse_ratio  # доля признаков из пути, которые можно \"вернуть\"\n",
        "        self.max_iter = max_iter\n",
        "        self.solver = solver\n",
        "    def fit(self, X, y):\n",
        "        # шаг 1: строим дерево для разбиений\n",
        "        self.tree_ = DecisionTreeClassifier(\n",
        "            max_depth=self.max_depth,\n",
        "            min_samples_leaf=self.min_samples_leaf,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        self.tree_.fit(X, y)\n",
        "        # шаг 2: распределяем объекты по листьям\n",
        "        leaf_ids = self.tree_.apply(X)\n",
        "        self.models_ = {}\n",
        "        self.classes_ = self.tree_.classes_\n",
        "        self.leaf_samples_ = {}\n",
        "        rng = np.random.RandomState(self.random_state)\n",
        "        for leaf in np.unique(leaf_ids):\n",
        "            mask = (leaf_ids == leaf)\n",
        "            self.leaf_samples_[leaf] = np.sum(mask)\n",
        "            # признаки, использованные на пути\n",
        "            path_features = list(self._get_features_on_path(leaf))\n",
        "            unused_features = [i for i in range(X.shape[1]) if i not in path_features]\n",
        "            # пропорция признаков из пути\n",
        "            k = max(1, int(len(path_features) * self.reuse_ratio)) if path_features else 0\n",
        "            reuse_features = rng.choice(path_features, size=k, replace=False).tolist() if k > 0 else []\n",
        "            final_features = unused_features + reuse_features\n",
        "            if not final_features:  # fallback\n",
        "                final_features = list(range(X.shape[1]))\n",
        "            X_leaf = X[mask][:, final_features]\n",
        "            y_leaf = y[mask]\n",
        "            if len(np.unique(y_leaf)) == 1:\n",
        "                # чистый лист\n",
        "                class_idx = np.where(self.classes_ == y_leaf[0])[0][0]\n",
        "                def dummy_model(X_input, c=class_idx):\n",
        "                    proba = np.zeros((X_input.shape[0], len(self.classes_)))\n",
        "                    proba[:, c] = 1.0\n",
        "                    return proba\n",
        "                self.models_[leaf] = (dummy_model, final_features, True, class_idx, None)\n",
        "            else:\n",
        "                model = make_pipeline(\n",
        "                    StandardScaler(),\n",
        "                    LogisticRegression(max_iter=self.max_iter, solver=self.solver)\n",
        "                )\n",
        "                model.fit(X_leaf, y_leaf)\n",
        "                coefs = model.named_steps[\"logisticregression\"].coef_\n",
        "                self.models_[leaf] = (model, final_features, False, None, coefs)\n",
        "        return self\n",
        "    def predict_proba(self, X):\n",
        "        leaf_ids = self.tree_.apply(X)\n",
        "        proba = np.zeros((X.shape[0], len(self.classes_)))\n",
        "        for leaf, (model, feats, is_dummy, _, _) in self.models_.items():\n",
        "            mask = (leaf_ids == leaf)\n",
        "            if np.any(mask):\n",
        "                X_leaf = X[mask][:, feats]\n",
        "                if is_dummy:\n",
        "                    proba[mask] = model(X_leaf)\n",
        "                else:\n",
        "                    proba[mask] = model.predict_proba(X_leaf)\n",
        "        return proba\n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.predict_proba(X), axis=1)\n",
        "    def _get_features_on_path(self, leaf_id):\n",
        "        \"\"\"Собрать все признаки, использованные на пути до данного листа\"\"\"\n",
        "        tree = self.tree_.tree_\n",
        "        path_features = set()\n",
        "        def recurse(node, path):\n",
        "            if tree.children_left[node] == -1 and tree.children_right[node] == -1:\n",
        "                if node == leaf_id:\n",
        "                    return path\n",
        "                return None\n",
        "            if tree.feature[node] >= 0:\n",
        "                left = tree.children_left[node]\n",
        "                right = tree.children_right[node]\n",
        "                if left != -1:\n",
        "                    res = recurse(left, path | {tree.feature[node]})\n",
        "                    if res is not None:\n",
        "                        return res\n",
        "                if right != -1:\n",
        "                    res = recurse(right, path | {tree.feature[node]})\n",
        "                    if res is not None:\n",
        "                        return res\n",
        "            return None\n",
        "        return recurse(0, set()) or set()\n",
        "    def print_leaf_stats(self, feature_names=None):\n",
        "        \"\"\"Вывести статистику по каждому листу\"\"\"\n",
        "        for leaf, (model, feats, is_dummy, class_idx, coefs) in self.models_.items():\n",
        "            print(\"=\"*60)\n",
        "            print(f\"Лист {leaf} | объектов: {self.leaf_samples_[leaf]}\")\n",
        "            used_feats = self._get_features_on_path(leaf)\n",
        "            if feature_names is not None:\n",
        "                used_feats = [feature_names[i] for i in used_feats]\n",
        "                feats_names = [feature_names[i] for i in feats]\n",
        "            else:\n",
        "                feats_names = feats\n",
        "            print(f\"  Использованные признаки на пути: {used_feats}\")\n",
        "            print(f\"  Признаки в логрег: {feats_names}\")\n",
        "            if is_dummy:\n",
        "                print(f\"  Модель: ЧИСТЫЙ ЛИСТ → всегда класс {self.classes_[class_idx]}\")\n",
        "            else:\n",
        "                print(\"  Модель: Логистическая регрессия (с масштабированием)\")\n",
        "                print(\"   Коэффициенты:\")\n",
        "                for i, c in enumerate(coefs[0]):\n",
        "                    fname = feats_names[i]\n",
        "                    print(f\"     {fname}: {c:.4f}\")\n"
      ],
      "metadata": {
        "id": "0G4zS-Hc6onQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticModelTree(max_depth=3, min_samples_leaf=30, random_state=42,\n",
        "                        reuse_ratio=0.2, max_iter=5000, solver=\"lbfgs\")\n",
        "clf.fit(X_train, y_train)\n",
        "#clf.print_leaf_stats(feature_names=load_breast_cancer().feature_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "IngOaUJ96qN8",
        "outputId": "211e0153-b31a-4094-ae9b-a93f21045a7a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticModelTree(min_samples_leaf=30, random_state=42, reuse_ratio=0.2)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticModelTree(min_samples_leaf=30, random_state=42, reuse_ratio=0.2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticModelTree</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticModelTree(min_samples_leaf=30, random_state=42, reuse_ratio=0.2)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratios = [0.0, 0.1, 0.2, 0.5]\n",
        "rows = []\n",
        "for r in ratios:\n",
        "    clf = LogisticModelTree(\n",
        "        max_depth=3,\n",
        "        min_samples_leaf=30,\n",
        "        random_state=42,\n",
        "        reuse_ratio=r,\n",
        "        max_iter=5000,\n",
        "        solver=\"lbfgs\"\n",
        "    )\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
        "    recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
        "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "    f2 = fbeta_score(y_test, y_pred, beta=2, average=\"weighted\")\n",
        "    rows.append({\n",
        "        \"reuse_ratio\": r,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"f2\": f2\n",
        "    })\n",
        "results_df = pd.DataFrame(rows)\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkF2HA5a6zgN",
        "outputId": "91fd18f0-92bf-429c-b2aa-182a699c73ca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   reuse_ratio  precision    recall        f1        f2\n",
            "0          0.0   0.976608  0.976608  0.976608  0.976608\n",
            "1          0.1   0.976608  0.976608  0.976608  0.976608\n",
            "2          0.2   0.976608  0.976608  0.976608  0.976608\n",
            "3          0.5   0.976608  0.976608  0.976608  0.976608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метрики одинаковые при любых reuse_ratio, что говорит о том, что дерево делит пространство так, что оставшихся признаков уже хватает для локальной логистической регрессии. Добавление/убавление 10–50% «старых» признаков не меняет картину — модель в листьях даёт одинаковые предсказания.\n",
        "\n",
        "Датасет Breast Cancer достаточно «лёгкий»: он линейно разделим и малошумный, поэтому гибрид быстро выходит на потолок ≈97–98% accuracy."
      ],
      "metadata": {
        "id": "ipNKrvP4-NBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем на синтетических данных"
      ],
      "metadata": {
        "id": "eNuS0C6BVTei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. создаём более сложный датасет\n",
        "X, y = make_classification(\n",
        "    n_samples=5000,\n",
        "    n_features=30,\n",
        "    n_informative=15,\n",
        "    n_redundant=10,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "# 2. модели для сравнения\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=5000),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
        "    \"XGBoost\": XGBClassifier(\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=4,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        eval_metric=\"logloss\",\n",
        "        use_label_encoder=False,\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "rows = []\n",
        "# 3. прогон LogisticModelTree с разными reuse_ratio\n",
        "for r in [0.0, 0.1, 0.2, 0.5]:\n",
        "    clf = LogisticModelTree(\n",
        "        max_depth=4,\n",
        "        min_samples_leaf=50,\n",
        "        random_state=42,\n",
        "        reuse_ratio=r,\n",
        "        max_iter=5000,\n",
        "        solver=\"lbfgs\"\n",
        "    )\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    rows.append({\n",
        "        \"Model\": f\"LMT (reuse={r})\",\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred, average=\"weighted\"),\n",
        "        \"Recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n",
        "        \"F1\": f1_score(y_test, y_pred, average=\"weighted\"),\n",
        "        \"F2\": fbeta_score(y_test, y_pred, beta=2, average=\"weighted\")\n",
        "    })\n",
        "# 4. прогон классических моделей\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    rows.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred, average=\"weighted\"),\n",
        "        \"Recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n",
        "        \"F1\": f1_score(y_test, y_pred, average=\"weighted\"),\n",
        "        \"F2\": fbeta_score(y_test, y_pred, beta=2, average=\"weighted\")\n",
        "    })\n",
        "# 5. выводим результаты\n",
        "results_df = pd.DataFrame(rows)\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izSc7Xm3VVuO",
        "outputId": "a3666965-ec97-497b-e8e0-9a61d46391bc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [13:27:20] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Model  Accuracy  Precision    Recall        F1        F2\n",
            "0      LMT (reuse=0.0)  0.878000   0.878116  0.878000  0.877992  0.877981\n",
            "1      LMT (reuse=0.1)  0.878000   0.878116  0.878000  0.877992  0.877981\n",
            "2      LMT (reuse=0.2)  0.878000   0.878116  0.878000  0.877992  0.877981\n",
            "3      LMT (reuse=0.5)  0.882000   0.882249  0.882000  0.881983  0.881959\n",
            "4  Logistic Regression  0.818667   0.818807  0.818667  0.818643  0.818635\n",
            "5        Random Forest  0.934667   0.934743  0.934667  0.934663  0.934655\n",
            "6              XGBoost  0.944667   0.944687  0.944667  0.944666  0.944664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Гибрид (LMT) заметно сильнее обычной Logistic Regression (+6 процентных пунктов), но сильно проигрывает ансамблям деревьев (RF и XGB).\n",
        "\n",
        "При reuse_ratio=0.5 результат немного лучше, чем при меньших значениях,то есть подмешивание части признаков ветвления действительно помогает.\n",
        "\n",
        "Random Forest и XGBoost на этом датасете показывают высокие результаты (93–94%).\n",
        "\n",
        "XGBoost чуть лучше, что типично для задач с нелинейной структурой и шумом.\n",
        "\n",
        "Выводы\n",
        "\n",
        "LMT уже даёт более гибкую модель, чем чистая логрег, но чтобы конкурировать с ансамблями, нужно либо глубже дерево, либо более «умный» выбор признаков в листьях (Можно добавить фича-селекшн по критериямв листе).\n",
        "\n",
        "При reuse_ratio=0.5 есть небольшой, но заметный прирост — так что идея рабочая."
      ],
      "metadata": {
        "id": "I9TpY7T0-cQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Добавим масштабирование перед построением регрессии в листе и фича-селекшн в модель. Подберем лучшие гиперпараметры с помощью Optuna."
      ],
      "metadata": {
        "id": "MV5UuM9h_mDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 1) Модель: LogisticModelTree с локальным feature selection ====\n",
        "class LogisticModelTree(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    Дерево разбиений + в листьях логистическая регрессия.\n",
        "    Улучшения:\n",
        "      - масштабирование признаков в каждом листе (StandardScaler),\n",
        "      - reuse_ratio: можно \"вернуть\" часть признаков, использованных на пути,\n",
        "      - per-leaf feature selection: выбор top-k признаков (по mutual information) из final_features.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 max_depth=3,\n",
        "                 min_samples_leaf=20,\n",
        "                 random_state=None,\n",
        "                 reuse_ratio=0.1,               # 0..1, доля признаков из пути, возвращаемых в лист\n",
        "                 topk_frac=1.0,                 # 0..1, доля final_features, оставляемая в листе (>=1 признак)\n",
        "                 C=1.0,                         # регуляризация логрег\n",
        "                 solver=\"lbfgs\",\n",
        "                 max_iter=5000):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.random_state = random_state\n",
        "        self.reuse_ratio = reuse_ratio\n",
        "        self.topk_frac = topk_frac\n",
        "        self.C = C\n",
        "        self.solver = solver\n",
        "        self.max_iter = max_iter\n",
        "    def fit(self, X, y):\n",
        "        self.tree_ = DecisionTreeClassifier(\n",
        "            max_depth=self.max_depth,\n",
        "            min_samples_leaf=self.min_samples_leaf,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        self.tree_.fit(X, y)\n",
        "        leaf_ids = self.tree_.apply(X)\n",
        "        self.models_ = {}\n",
        "        self.classes_ = self.tree_.classes_\n",
        "        self.leaf_samples_ = {}\n",
        "        rng = np.random.RandomState(self.random_state)\n",
        "        n_features = X.shape[1]\n",
        "        for leaf in np.unique(leaf_ids):\n",
        "            mask = (leaf_ids == leaf)\n",
        "            self.leaf_samples_[leaf] = int(np.sum(mask))\n",
        "            # признаки на пути к листу\n",
        "            path_features = list(self._get_features_on_path(leaf))\n",
        "            unused_features = [i for i in range(n_features) if i not in path_features]\n",
        "            # вернуть часть \"деревянных\" признаков\n",
        "            k_reuse = max(0, int(len(path_features) * float(self.reuse_ratio))) if path_features else 0\n",
        "            reuse_features = rng.choice(path_features, size=k_reuse, replace=False).tolist() if k_reuse > 0 else []\n",
        "            final_features = unused_features + reuse_features\n",
        "            if not final_features:   # fallback\n",
        "                final_features = list(range(n_features))\n",
        "            X_leaf_full = X[mask]\n",
        "            y_leaf = y[mask]\n",
        "            # \"чистый\" лист -> детерминистическая модель\n",
        "            if len(np.unique(y_leaf)) == 1:\n",
        "                class_idx = int(np.where(self.classes_ == y_leaf[0])[0][0])\n",
        "                def dummy_model(X_input, c=class_idx, n_classes=len(self.classes_)):\n",
        "                    proba = np.zeros((X_input.shape[0], n_classes))\n",
        "                    proba[:, c] = 1.0\n",
        "                    return proba\n",
        "                self.models_[leaf] = (dummy_model, final_features, True, class_idx, None, None)\n",
        "                continue\n",
        "            # ---- локальный feature selection по mutual information ----\n",
        "            # считаем важности только по final_features\n",
        "            X_sub = X_leaf_full[:, final_features]\n",
        "            # mutual_info_classif устойчив к масштабам; дискретизации не нужно\n",
        "            mi = mutual_info_classif(X_sub, y_leaf, random_state=self.random_state)\n",
        "            order = np.argsort(mi)[::-1]  # убыв. важность\n",
        "            k_top = max(1, int(ceil(len(final_features) * float(self.topk_frac))))\n",
        "            keep_idx = order[:k_top]\n",
        "            selected_features = [final_features[i] for i in keep_idx]\n",
        "            # обучаем пайплайн: скейлер + логрег\n",
        "            X_leaf = X_leaf_full[:, selected_features]\n",
        "            model = make_pipeline(\n",
        "                StandardScaler(),\n",
        "                LogisticRegression(\n",
        "                    max_iter=self.max_iter,\n",
        "                    solver=self.solver,\n",
        "                    C=self.C\n",
        "                )\n",
        "            )\n",
        "            model.fit(X_leaf, y_leaf)\n",
        "            coefs = model.named_steps[\"logisticregression\"].coef_\n",
        "            self.models_[leaf] = (model, selected_features, False, None, coefs, mi)\n",
        "        return self\n",
        "    def predict_proba(self, X):\n",
        "        leaf_ids = self.tree_.apply(X)\n",
        "        proba = np.zeros((X.shape[0], len(self.classes_)))\n",
        "        for leaf, (model, feats, is_dummy, _, _, _) in self.models_.items():\n",
        "            mask = (leaf_ids == leaf)\n",
        "            if not np.any(mask):\n",
        "                continue\n",
        "            X_leaf = X[mask][:, feats]\n",
        "            if is_dummy:\n",
        "                proba[mask] = model(X_leaf)\n",
        "            else:\n",
        "                proba[mask] = model.predict_proba(X_leaf)\n",
        "        return proba\n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.predict_proba(X), axis=1)\n",
        "    def _get_features_on_path(self, leaf_id):\n",
        "        tree = self.tree_.tree_\n",
        "        def recurse(node, used):\n",
        "            # лист\n",
        "            if tree.children_left[node] == -1 and tree.children_right[node] == -1:\n",
        "                return used if node == leaf_id else None\n",
        "            if tree.feature[node] >= 0:\n",
        "                left = tree.children_left[node]\n",
        "                right = tree.children_right[node]\n",
        "                if left != -1:\n",
        "                    r = recurse(left, used | {int(tree.feature[node])})\n",
        "                    if r is not None:\n",
        "                        return r\n",
        "                if right != -1:\n",
        "                    r = recurse(right, used | {int(tree.feature[node])})\n",
        "                    if r is not None:\n",
        "                        return r\n",
        "            return None\n",
        "        res = recurse(0, set())\n",
        "        return res or set()\n",
        "    def print_leaf_stats(self, feature_names=None, show_top=10):\n",
        "        for leaf, (model, feats, is_dummy, class_idx, coefs, mi) in self.models_.items():\n",
        "            print(\"=\"*70)\n",
        "            print(f\"Лист {leaf} | объектов: {self.leaf_samples_[leaf]}\")\n",
        "            used_feats = self._get_features_on_path(leaf)\n",
        "            if feature_names is not None:\n",
        "                used_feats_names = [feature_names[i] for i in used_feats]\n",
        "                feats_names = [feature_names[i] for i in feats]\n",
        "            else:\n",
        "                used_feats_names = list(used_feats)\n",
        "                feats_names = feats\n",
        "            print(f\"  Признаки на пути: {used_feats_names}\")\n",
        "            print(f\"  Признаки в логрег (после selection): {feats_names[:show_top]}{' ...' if len(feats_names)>show_top else ''}\")\n",
        "            if is_dummy:\n",
        "                print(f\"  Модель: ЧИСТЫЙ ЛИСТ → класс {self.classes_[class_idx]}\")\n",
        "            else:\n",
        "                print(\"  Модель: Логистическая регрессия (скейлер + L2)\")\n",
        "                print(f\"   Кол-во признаков в листе: {len(feats_names)}\")\n",
        "                if coefs is not None:\n",
        "                    for i, c in enumerate(coefs[0][:min(len(feats_names), show_top)]):\n",
        "                        print(f\"     {feats_names[i]}: {c:.4f}\")\n",
        "                if mi is not None:\n",
        "                    print(\"   (MI использовалось для отбора признаков)\")\n",
        "# ==== 2) Optuna: подбор гиперпараметров LMT ====\n",
        "# Если optuna не установлена — установи: pip install optuna\n",
        "def tune_lmt_with_optuna(X, y, n_trials=50, random_state=42):\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
        "    scorer = make_scorer(f1_score, average=\"weighted\")\n",
        "    def objective(trial: optuna.Trial):\n",
        "        params = {\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 7),\n",
        "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 10, 200),\n",
        "            \"reuse_ratio\": trial.suggest_float(\"reuse_ratio\", 0.0, 0.8),\n",
        "            \"topk_frac\": trial.suggest_float(\"topk_frac\", 0.2, 1.0),\n",
        "            \"C\": trial.suggest_float(\"C\", 1e-3, 10.0, log=True),\n",
        "            \"solver\": trial.suggest_categorical(\"solver\", [\"lbfgs\", \"saga\"]),\n",
        "            \"max_iter\": 5000,\n",
        "            \"random_state\": random_state,\n",
        "        }\n",
        "        # Saga поддерживает l2, всё ок; на маленьких листах может быть быстрее.\n",
        "        model = LogisticModelTree(**params)\n",
        "        scores = cross_val_score(model, X, y, scoring=scorer, cv=skf, n_jobs=-1)\n",
        "        return float(np.mean(scores))\n",
        "    study = optuna.create_study(direction=\"maximize\",\n",
        "                                sampler=optuna.samplers.TPESampler(seed=random_state),\n",
        "                                pruner=optuna.pruners.MedianPruner(n_warmup_steps=10))\n",
        "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
        "    return study\n",
        "# ==== 3) Запуск тюнинга и финальная оценка ====\n",
        "# Если у тебя уже есть X_train/X_test/y_train/y_test — используй их.\n",
        "# Иначе раскомментируй блок генерации синтетики в самом низу.\n",
        "study = tune_lmt_with_optuna(X_train, y_train, n_trials=60, random_state=42)\n",
        "best_params = study.best_params\n",
        "print(\"Best params (Optuna):\", best_params)\n",
        "# дообучаем на train, проверяем на test\n",
        "lmt_best = LogisticModelTree(**{**best_params, \"max_iter\": 5000, \"random_state\": 42})\n",
        "lmt_best.fit(X_train, y_train)\n",
        "y_pred_lmt = lmt_best.predict(X_test)\n",
        "def metrics_row(name, y_true, y_pred):\n",
        "    return {\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"Precision\": precision_score(y_true, y_pred, average=\"weighted\"),\n",
        "        \"Recall\": recall_score(y_true, y_pred, average=\"weighted\"),\n",
        "        \"F1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
        "        \"F2\": fbeta_score(y_true, y_pred, beta=2, average=\"weighted\"),\n",
        "    }\n",
        "rows = [metrics_row(\"LMT (Optuna)\", y_test, y_pred_lmt)]\n",
        "# ==== 4) Бейзлайны: LogisticRegression / RandomForest / XGBoost ====\n",
        "try:\n",
        "    has_xgb = True\n",
        "except Exception:\n",
        "    has_xgb = False\n",
        "base_lr = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    LogisticRegression(max_iter=5000, C=1.0, solver=\"lbfgs\")\n",
        ").fit(X_train, y_train)\n",
        "rows.append(metrics_row(\"Logistic Regression\", y_test, base_lr.predict(X_test)))\n",
        "rf = RandomForestClassifier(n_estimators=400, max_depth=None, min_samples_leaf=1,\n",
        "                            random_state=42, n_jobs=-1).fit(X_train, y_train)\n",
        "rows.append(metrics_row(\"Random Forest\", y_test, rf.predict(X_test)))\n",
        "if has_xgb:\n",
        "    xgb = XGBClassifier(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=5,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_lambda=1.0,\n",
        "        eval_metric=\"logloss\",\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ).fit(X_train, y_train)\n",
        "    rows.append(metrics_row(\"XGBoost\", y_test, xgb.predict(X_test)))\n",
        "results = pd.DataFrame(rows).sort_values(\"Accuracy\", ascending=False)\n",
        "print(results)\n",
        "# (опционально) быстрый просмотр важности гиперов в Optuna:\n",
        "try:\n",
        "    fig = viz.plot_param_importances(study)\n",
        "    # fig.show()  # в Jupyter можно показать интерактивно\n",
        "except Exception:\n",
        "    pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8H7DRcJnX1i-",
        "outputId": "912374f8-aeed-4c0e-a01b-f95081f1e1bb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-09-19 13:27:23,474] A new study created in memory with name: no-name-666c979f-9a1a-40ca-8b71-d2d769458cd7\n",
            "[I 2025-09-19 13:27:29,589] Trial 0 finished with value: 0.7922722535939213 and parameters: {'max_depth': 4, 'min_samples_leaf': 191, 'reuse_ratio': 0.585595153449124, 'topk_frac': 0.6789267873576292, 'C': 0.004207988669606638, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.7922722535939213.\n",
            "[I 2025-09-19 13:27:35,194] Trial 1 finished with value: 0.847630010515865 and parameters: {'max_depth': 7, 'min_samples_leaf': 124, 'reuse_ratio': 0.5664580622368364, 'topk_frac': 0.21646759543664196, 'C': 7.579479953348009, 'solver': 'lbfgs'}. Best is trial 1 with value: 0.847630010515865.\n",
            "[I 2025-09-19 13:27:38,600] Trial 2 finished with value: 0.8356066190706523 and parameters: {'max_depth': 3, 'min_samples_leaf': 45, 'reuse_ratio': 0.2433937943676302, 'topk_frac': 0.6198051453057902, 'C': 0.05342937261279776, 'solver': 'saga'}. Best is trial 1 with value: 0.847630010515865.\n",
            "[I 2025-09-19 13:27:41,307] Trial 3 finished with value: 0.8402343082835783 and parameters: {'max_depth': 2, 'min_samples_leaf': 65, 'reuse_ratio': 0.2930894746349534, 'topk_frac': 0.5648559873736287, 'C': 1.382623217936987, 'solver': 'saga'}. Best is trial 1 with value: 0.847630010515865.\n",
            "[I 2025-09-19 13:27:48,629] Trial 4 finished with value: 0.7921939684055569 and parameters: {'max_depth': 5, 'min_samples_leaf': 18, 'reuse_ratio': 0.48603588152115074, 'topk_frac': 0.33641929894983325, 'C': 0.0018205657658407262, 'solver': 'saga'}. Best is trial 1 with value: 0.847630010515865.\n",
            "[I 2025-09-19 13:27:54,822] Trial 5 finished with value: 0.8567622515091842 and parameters: {'max_depth': 6, 'min_samples_leaf': 68, 'reuse_ratio': 0.07813769120510711, 'topk_frac': 0.7473864212097256, 'C': 0.057624872164786026, 'solver': 'saga'}. Best is trial 5 with value: 0.8567622515091842.\n",
            "[I 2025-09-19 13:27:56,981] Trial 6 finished with value: 0.8218960662239849 and parameters: {'max_depth': 2, 'min_samples_leaf': 183, 'reuse_ratio': 0.20702398528001353, 'topk_frac': 0.7300178274831857, 'C': 0.017654048052495083, 'solver': 'saga'}. Best is trial 5 with value: 0.8567622515091842.\n",
            "[I 2025-09-19 13:28:01,204] Trial 7 finished with value: 0.8639795866326387 and parameters: {'max_depth': 3, 'min_samples_leaf': 195, 'reuse_ratio': 0.6201062586888917, 'topk_frac': 0.9515991532513512, 'C': 3.7958531426706403, 'solver': 'saga'}. Best is trial 7 with value: 0.8639795866326387.\n",
            "[I 2025-09-19 13:28:04,042] Trial 8 finished with value: 0.8173388789147955 and parameters: {'max_depth': 2, 'min_samples_leaf': 47, 'reuse_ratio': 0.03618183112843045, 'topk_frac': 0.4602642646106115, 'C': 0.03586816498627549, 'solver': 'saga'}. Best is trial 7 with value: 0.8639795866326387.\n",
            "[I 2025-09-19 13:28:08,119] Trial 9 finished with value: 0.8390168688619314 and parameters: {'max_depth': 4, 'min_samples_leaf': 63, 'reuse_ratio': 0.4341568665265988, 'topk_frac': 0.31273937997981016, 'C': 1.6172900811143154, 'solver': 'saga'}. Best is trial 7 with value: 0.8639795866326387.\n",
            "[I 2025-09-19 13:28:12,561] Trial 10 finished with value: 0.8679285220519439 and parameters: {'max_depth': 5, 'min_samples_leaf': 143, 'reuse_ratio': 0.7756296886302927, 'topk_frac': 0.9641119495811593, 'C': 0.4050104259141603, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.8679285220519439.\n",
            "[I 2025-09-19 13:28:18,442] Trial 11 finished with value: 0.868239821448354 and parameters: {'max_depth': 5, 'min_samples_leaf': 155, 'reuse_ratio': 0.771094370186789, 'topk_frac': 0.9979547888017002, 'C': 0.5325924546877745, 'solver': 'lbfgs'}. Best is trial 11 with value: 0.868239821448354.\n",
            "[I 2025-09-19 13:28:22,893] Trial 12 finished with value: 0.86734908076739 and parameters: {'max_depth': 5, 'min_samples_leaf': 145, 'reuse_ratio': 0.7972212949427269, 'topk_frac': 0.9763241734715128, 'C': 0.4133463366645983, 'solver': 'lbfgs'}. Best is trial 11 with value: 0.868239821448354.\n",
            "[I 2025-09-19 13:28:27,620] Trial 13 finished with value: 0.8685003401159346 and parameters: {'max_depth': 6, 'min_samples_leaf': 152, 'reuse_ratio': 0.786262764053931, 'topk_frac': 0.8678732061567765, 'C': 0.27308349581482794, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.8685003401159346.\n",
            "[I 2025-09-19 13:28:32,640] Trial 14 finished with value: 0.8682506156050416 and parameters: {'max_depth': 7, 'min_samples_leaf': 159, 'reuse_ratio': 0.6809959889356856, 'topk_frac': 0.8984886269914918, 'C': 0.22769339956247306, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.8685003401159346.\n",
            "[I 2025-09-19 13:28:38,020] Trial 15 finished with value: 0.8642278694177786 and parameters: {'max_depth': 7, 'min_samples_leaf': 106, 'reuse_ratio': 0.6746126159618873, 'topk_frac': 0.8397548798675294, 'C': 0.17661788962270084, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.8685003401159346.\n",
            "[I 2025-09-19 13:28:43,614] Trial 16 finished with value: 0.825573176399726 and parameters: {'max_depth': 6, 'min_samples_leaf': 163, 'reuse_ratio': 0.6833003874619301, 'topk_frac': 0.8399117369736302, 'C': 0.010766193586447818, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.8685003401159346.\n",
            "[I 2025-09-19 13:28:48,817] Trial 17 finished with value: 0.8676487916228164 and parameters: {'max_depth': 6, 'min_samples_leaf': 112, 'reuse_ratio': 0.5201585700887333, 'topk_frac': 0.8751012346204443, 'C': 0.20809622306598385, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.8685003401159346.\n",
            "[I 2025-09-19 13:28:52,792] Trial 18 finished with value: 0.8759147694982957 and parameters: {'max_depth': 7, 'min_samples_leaf': 174, 'reuse_ratio': 0.3715924269750973, 'topk_frac': 0.7793326800114807, 'C': 1.1072448678802878, 'solver': 'lbfgs'}. Best is trial 18 with value: 0.8759147694982957.\n",
            "[I 2025-09-19 13:28:58,106] Trial 19 finished with value: 0.8773396613578152 and parameters: {'max_depth': 6, 'min_samples_leaf': 175, 'reuse_ratio': 0.35748122127911736, 'topk_frac': 0.7820979668396375, 'C': 1.1349835828662918, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:29:02,588] Trial 20 finished with value: 0.8747676733451513 and parameters: {'max_depth': 7, 'min_samples_leaf': 174, 'reuse_ratio': 0.3432829204292509, 'topk_frac': 0.7403522856316538, 'C': 1.3358219015117803, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:29:06,438] Trial 21 finished with value: 0.8747735461559056 and parameters: {'max_depth': 7, 'min_samples_leaf': 177, 'reuse_ratio': 0.3488798668578241, 'topk_frac': 0.7892450055472878, 'C': 1.089703896050741, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:29:12,707] Trial 22 finished with value: 0.8719217294612015 and parameters: {'max_depth': 7, 'min_samples_leaf': 129, 'reuse_ratio': 0.4098341179441685, 'topk_frac': 0.7940606675621515, 'C': 3.042560151816738, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:29:16,392] Trial 23 finished with value: 0.8662192830307525 and parameters: {'max_depth': 6, 'min_samples_leaf': 177, 'reuse_ratio': 0.1598245626318011, 'topk_frac': 0.6266274593571007, 'C': 0.8014470596497347, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:29:20,120] Trial 24 finished with value: 0.8682238757690982 and parameters: {'max_depth': 7, 'min_samples_leaf': 198, 'reuse_ratio': 0.3570321107673942, 'topk_frac': 0.5287942879344764, 'C': 9.818758335453511, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:29:25,551] Trial 25 finished with value: 0.8770717306462679 and parameters: {'max_depth': 6, 'min_samples_leaf': 168, 'reuse_ratio': 0.32074501137549105, 'topk_frac': 0.6796476664281874, 'C': 3.0905181849143526, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:29:30,310] Trial 26 finished with value: 0.8673472074061268 and parameters: {'max_depth': 6, 'min_samples_leaf': 131, 'reuse_ratio': 0.27537697263613337, 'topk_frac': 0.6726228551296527, 'C': 2.859401788700853, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:29:37,032] Trial 27 finished with value: 0.8593899702459815 and parameters: {'max_depth': 6, 'min_samples_leaf': 92, 'reuse_ratio': 0.4685246849032527, 'topk_frac': 0.4598004078367339, 'C': 5.507822098112298, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:29:41,014] Trial 28 finished with value: 0.8610337964908152 and parameters: {'max_depth': 5, 'min_samples_leaf': 166, 'reuse_ratio': 0.12965482357891778, 'topk_frac': 0.680493831368393, 'C': 0.10042418370109642, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:29:44,591] Trial 29 finished with value: 0.8748080959790728 and parameters: {'max_depth': 4, 'min_samples_leaf': 200, 'reuse_ratio': 0.39065612817788437, 'topk_frac': 0.6620731732309563, 'C': 2.3205523306828715, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:29:51,476] Trial 30 finished with value: 0.8679718380480134 and parameters: {'max_depth': 6, 'min_samples_leaf': 89, 'reuse_ratio': 0.3097383846205359, 'topk_frac': 0.7801292397595313, 'C': 0.7778437767765636, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:29:55,180] Trial 31 finished with value: 0.873664730771797 and parameters: {'max_depth': 4, 'min_samples_leaf': 186, 'reuse_ratio': 0.3905734246689501, 'topk_frac': 0.6962884522094629, 'C': 2.1866383168697663, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:29:58,003] Trial 32 finished with value: 0.867398131299997 and parameters: {'max_depth': 3, 'min_samples_leaf': 199, 'reuse_ratio': 0.22084616055932374, 'topk_frac': 0.6400362738620134, 'C': 4.416434003966077, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:30:02,571] Trial 33 finished with value: 0.8742540358857257 and parameters: {'max_depth': 4, 'min_samples_leaf': 169, 'reuse_ratio': 0.5382677387861987, 'topk_frac': 0.5824497435602637, 'C': 6.643474324345532, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:30:06,770] Trial 34 finished with value: 0.8659301235194043 and parameters: {'max_depth': 4, 'min_samples_leaf': 190, 'reuse_ratio': 0.4624378410637951, 'topk_frac': 0.548239060793387, 'C': 1.9236508814039184, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:30:09,591] Trial 35 finished with value: 0.8522525594226679 and parameters: {'max_depth': 3, 'min_samples_leaf': 139, 'reuse_ratio': 0.2762008840222645, 'topk_frac': 0.5042019423452664, 'C': 0.7195572496696704, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:30:13,429] Trial 36 finished with value: 0.8728010319252647 and parameters: {'max_depth': 5, 'min_samples_leaf': 183, 'reuse_ratio': 0.3939714492078024, 'topk_frac': 0.7021268657273326, 'C': 9.313805409513794, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:30:25,454] Trial 37 finished with value: 0.848492514725681 and parameters: {'max_depth': 7, 'min_samples_leaf': 12, 'reuse_ratio': 0.3089762817119999, 'topk_frac': 0.6396238348532701, 'C': 0.11508223208505455, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:30:30,069] Trial 38 finished with value: 0.7760611289452735 and parameters: {'max_depth': 5, 'min_samples_leaf': 184, 'reuse_ratio': 0.24246357493341636, 'topk_frac': 0.8148332557969378, 'C': 0.0020801864782593214, 'solver': 'saga'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:30:35,178] Trial 39 finished with value: 0.8676406275921584 and parameters: {'max_depth': 6, 'min_samples_leaf': 119, 'reuse_ratio': 0.17789430245949447, 'topk_frac': 0.9110392532521808, 'C': 2.6016375516089822, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:30:39,723] Trial 40 finished with value: 0.875332211985306 and parameters: {'max_depth': 7, 'min_samples_leaf': 171, 'reuse_ratio': 0.43563171451228794, 'topk_frac': 0.7292502222877018, 'C': 1.1444157625781457, 'solver': 'saga'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:30:46,275] Trial 41 finished with value: 0.8744616431932972 and parameters: {'max_depth': 7, 'min_samples_leaf': 171, 'reuse_ratio': 0.4439389549139615, 'topk_frac': 0.7497759390700885, 'C': 1.1983374343983888, 'solver': 'saga'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:30:52,061] Trial 42 finished with value: 0.8662046985356451 and parameters: {'max_depth': 7, 'min_samples_leaf': 150, 'reuse_ratio': 0.5058849096351641, 'topk_frac': 0.7196564656305189, 'C': 4.069345480473201, 'solver': 'saga'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:30:58,525] Trial 43 finished with value: 0.8710927807951172 and parameters: {'max_depth': 7, 'min_samples_leaf': 191, 'reuse_ratio': 0.37105638689479103, 'topk_frac': 0.6062314265432109, 'C': 1.9001150948282142, 'solver': 'saga'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:31:03,051] Trial 44 finished with value: 0.8673703724121558 and parameters: {'max_depth': 6, 'min_samples_leaf': 161, 'reuse_ratio': 0.5639805021418285, 'topk_frac': 0.7575971674573674, 'C': 0.46165485666388567, 'solver': 'saga'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:31:06,391] Trial 45 finished with value: 0.865103996159178 and parameters: {'max_depth': 3, 'min_samples_leaf': 177, 'reuse_ratio': 0.4125393167769691, 'topk_frac': 0.6675659352994935, 'C': 0.976245631754959, 'solver': 'saga'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:31:11,728] Trial 46 finished with value: 0.8719376897860934 and parameters: {'max_depth': 4, 'min_samples_leaf': 200, 'reuse_ratio': 0.43588502320303957, 'topk_frac': 0.8111591297505119, 'C': 0.5934648266995096, 'solver': 'saga'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:31:20,710] Trial 47 finished with value: 0.8636790326719692 and parameters: {'max_depth': 6, 'min_samples_leaf': 34, 'reuse_ratio': 0.33109465525767495, 'topk_frac': 0.7198335743999765, 'C': 5.057191730627156, 'solver': 'saga'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:31:26,563] Trial 48 finished with value: 0.8650516268078106 and parameters: {'max_depth': 7, 'min_samples_leaf': 136, 'reuse_ratio': 0.26851923462798977, 'topk_frac': 0.928028320905938, 'C': 0.37524247456730275, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:31:30,702] Trial 49 finished with value: 0.8307065043071249 and parameters: {'max_depth': 5, 'min_samples_leaf': 156, 'reuse_ratio': 0.6005438124925879, 'topk_frac': 0.2621535039719005, 'C': 0.050905524466484525, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:31:36,686] Trial 50 finished with value: 0.8659104012053224 and parameters: {'max_depth': 6, 'min_samples_leaf': 147, 'reuse_ratio': 0.48361098337339753, 'topk_frac': 0.8478975816091678, 'C': 1.5566177525263156, 'solver': 'saga'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:31:40,878] Trial 51 finished with value: 0.8753523321198123 and parameters: {'max_depth': 7, 'min_samples_leaf': 178, 'reuse_ratio': 0.3645786734002865, 'topk_frac': 0.7645969564370823, 'C': 1.2772513503020198, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:31:44,672] Trial 52 finished with value: 0.8770998663625672 and parameters: {'max_depth': 7, 'min_samples_leaf': 191, 'reuse_ratio': 0.3842434560808501, 'topk_frac': 0.7601194947697968, 'C': 3.063729523742541, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:31:49,325] Trial 53 finished with value: 0.8765352581204905 and parameters: {'max_depth': 7, 'min_samples_leaf': 166, 'reuse_ratio': 0.31282578236515035, 'topk_frac': 0.7711370172423506, 'C': 3.4960491658041932, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:31:53,813] Trial 54 finished with value: 0.875954085376185 and parameters: {'max_depth': 7, 'min_samples_leaf': 182, 'reuse_ratio': 0.3160805438455304, 'topk_frac': 0.7677946577245606, 'C': 3.382056955237783, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:31:57,961] Trial 55 finished with value: 0.8714010686646871 and parameters: {'max_depth': 7, 'min_samples_leaf': 163, 'reuse_ratio': 0.3170655135375736, 'topk_frac': 0.8190718598455078, 'C': 3.169436334886666, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:32:01,971] Trial 56 finished with value: 0.8722285141841993 and parameters: {'max_depth': 7, 'min_samples_leaf': 190, 'reuse_ratio': 0.233661955426085, 'topk_frac': 0.8784379126854336, 'C': 6.504790681719505, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:32:06,676] Trial 57 finished with value: 0.8742200385753514 and parameters: {'max_depth': 6, 'min_samples_leaf': 180, 'reuse_ratio': 0.286107894974272, 'topk_frac': 0.7806786079143302, 'C': 3.351902703574009, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:32:10,900] Trial 58 finished with value: 0.8676718579802074 and parameters: {'max_depth': 7, 'min_samples_leaf': 155, 'reuse_ratio': 0.33568985015306924, 'topk_frac': 0.8505971524683833, 'C': 7.508463772292072, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n",
            "[I 2025-09-19 13:32:15,228] Trial 59 finished with value: 0.8295517577104917 and parameters: {'max_depth': 7, 'min_samples_leaf': 166, 'reuse_ratio': 0.25453883167680424, 'topk_frac': 0.808141453134178, 'C': 0.015772907363763883, 'solver': 'lbfgs'}. Best is trial 19 with value: 0.8773396613578152.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params (Optuna): {'max_depth': 6, 'min_samples_leaf': 175, 'reuse_ratio': 0.35748122127911736, 'topk_frac': 0.7820979668396375, 'C': 1.1349835828662918, 'solver': 'lbfgs'}\n",
            "                 Model  Accuracy  Precision    Recall        F1        F2\n",
            "3              XGBoost  0.955333   0.955341  0.955333  0.955333  0.955332\n",
            "2        Random Forest  0.934667   0.934715  0.934667  0.934664  0.934659\n",
            "0         LMT (Optuna)  0.872667   0.872722  0.872667  0.872663  0.872658\n",
            "1  Logistic Regression  0.818667   0.818807  0.818667  0.818643  0.818635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost ожидаемо лидер, он идеально справляется с нелинейными разделяющими поверхностями на синтетике.\n",
        "\n",
        "RandomForest чуть слабее, но тоже близко.\n",
        "\n",
        "LMT (Optuna): лучше, чем глобальная логрег (+5%), но заметно отстаёт от ансамблей.\n",
        "\n",
        "Logistic Regression в чистом виде — самая простая и наименее подходящая модель для этого датасета.\n",
        "\n",
        "LMT реально улучшает линейную модель, сохраняя интерпретируемость и гибкость, но ансамбли деревьев остаются лучшими на сложных данных.\n",
        "Оптимизация гиперпараметров дала неплохой результат, но сам класс моделей (LMT) пока ограничен по мощности.\n",
        "\n",
        "Возможные апгрейды LMT\n",
        "добавить регуляризацию на уровне признаков в листьях (L1 для отбора, ElasticNet);\n",
        "попробовать бустинг из LMT (как GradientBoosting, но листья = логреги);\n",
        "попробовать беггинг;\n",
        "попробовать более глубокие деревья + уменьшить min_samples_leaf, чтобы сделать более локальные логреги."
      ],
      "metadata": {
        "id": "FCwmjulU_5dO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Но сначала попробуем улучшения и на реальном датасете Wine\n"
      ],
      "metadata": {
        "id": "tX_h15dKbmW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticModelTree(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self,\n",
        "                 max_depth=3,\n",
        "                 min_samples_leaf=20,\n",
        "                 random_state=None,\n",
        "                 reuse_ratio=0.1,\n",
        "                 topk_frac=1.0,\n",
        "                 C=1.0,\n",
        "                 solver=\"lbfgs\",\n",
        "                 max_iter=5000):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.random_state = random_state\n",
        "        self.reuse_ratio = reuse_ratio\n",
        "        self.topk_frac = topk_frac\n",
        "        self.C = C\n",
        "        self.solver = solver\n",
        "        self.max_iter = max_iter\n",
        "    def fit(self, X, y):\n",
        "        self.tree_ = DecisionTreeClassifier(\n",
        "            max_depth=self.max_depth,\n",
        "            min_samples_leaf=self.min_samples_leaf,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        self.tree_.fit(X, y)\n",
        "        leaf_ids = self.tree_.apply(X)\n",
        "        self.models_ = {}\n",
        "        self.classes_ = np.array(self.tree_.classes_)\n",
        "        self.class_to_index_ = {c: i for i, c in enumerate(self.classes_)}\n",
        "        self.leaf_samples_ = {}\n",
        "        rng = np.random.RandomState(self.random_state)\n",
        "        n_features = X.shape[1]\n",
        "        for leaf in np.unique(leaf_ids):\n",
        "            mask = (leaf_ids == leaf)\n",
        "            self.leaf_samples_[leaf] = int(np.sum(mask))\n",
        "            path_features = list(self._get_features_on_path(leaf))\n",
        "            unused_features = [i for i in range(n_features) if i not in path_features]\n",
        "            k_reuse = max(0, int(len(path_features) * float(self.reuse_ratio))) if path_features else 0\n",
        "            reuse_features = rng.choice(path_features, size=k_reuse, replace=False).tolist() if k_reuse > 0 else []\n",
        "            final_features = unused_features + reuse_features\n",
        "            if not final_features:\n",
        "                final_features = list(range(n_features))\n",
        "            X_leaf_full = X[mask]\n",
        "            y_leaf = y[mask]\n",
        "            # чистый лист\n",
        "            unique_leaf_classes = np.unique(y_leaf)\n",
        "            if len(unique_leaf_classes) == 1:\n",
        "                class_idx = int(self.class_to_index_[unique_leaf_classes[0]])\n",
        "                def dummy_model(X_input, c=class_idx, n_classes=len(self.classes_)):\n",
        "                    proba = np.zeros((X_input.shape[0], n_classes))\n",
        "                    proba[:, c] = 1.0\n",
        "                    return proba\n",
        "                self.models_[leaf] = {\n",
        "                    \"model\": dummy_model,\n",
        "                    \"feats\": final_features,\n",
        "                    \"is_dummy\": True,\n",
        "                    \"leaf_classes\": np.array([unique_leaf_classes[0]]),\n",
        "                    \"coefs\": None,\n",
        "                    \"mi\": None,\n",
        "                }\n",
        "                continue\n",
        "            # локальный feature selection по MI (если объектов совсем мало — пропускаем селекцию)\n",
        "            X_sub = X_leaf_full[:, final_features]\n",
        "            if X_sub.shape[0] >= 5:\n",
        "                mi = mutual_info_classif(X_sub, y_leaf, random_state=self.random_state)\n",
        "                order = np.argsort(mi)[::-1]\n",
        "                k_top = max(1, int(ceil(len(final_features) * float(self.topk_frac))))\n",
        "                keep_idx = order[:k_top]\n",
        "                selected_features = [final_features[i] for i in keep_idx]\n",
        "            else:\n",
        "                mi = None\n",
        "                selected_features = final_features\n",
        "            X_leaf = X_leaf_full[:, selected_features]\n",
        "            pipe = make_pipeline(\n",
        "                StandardScaler(),\n",
        "                LogisticRegression(\n",
        "                    max_iter=self.max_iter,\n",
        "                    solver=self.solver,\n",
        "                    C=self.C,\n",
        "                    multi_class=\"auto\"\n",
        "                )\n",
        "            )\n",
        "            pipe.fit(X_leaf, y_leaf)\n",
        "            lr = pipe.named_steps[\"logisticregression\"]\n",
        "            self.models_[leaf] = {\n",
        "                \"model\": pipe,\n",
        "                \"feats\": selected_features,\n",
        "                \"is_dummy\": False,\n",
        "                \"leaf_classes\": np.array(lr.classes_),  # важный момент!\n",
        "                \"coefs\": lr.coef_,\n",
        "                \"mi\": mi,\n",
        "            }\n",
        "        return self\n",
        "    def predict_proba(self, X):\n",
        "        leaf_ids = self.tree_.apply(X)\n",
        "        proba = np.zeros((X.shape[0], len(self.classes_)))\n",
        "        for leaf, blob in self.models_.items():\n",
        "            mask = (leaf_ids == leaf)\n",
        "            if not np.any(mask):\n",
        "                continue\n",
        "            feats = blob[\"feats\"]\n",
        "            X_leaf = X[mask][:, feats]\n",
        "            if blob[\"is_dummy\"]:\n",
        "                proba[mask] = blob[\"model\"](X_leaf)\n",
        "            else:\n",
        "                local_proba = blob[\"model\"].predict_proba(X_leaf)  # shape: [n, n_leaf_classes]\n",
        "                leaf_classes = blob[\"leaf_classes\"]\n",
        "                # распределяем по глобальным классам\n",
        "                tmp = np.zeros((local_proba.shape[0], len(self.classes_)))\n",
        "                for j, cls in enumerate(leaf_classes):\n",
        "                    gidx = self.class_to_index_[cls]\n",
        "                    tmp[:, gidx] = local_proba[:, j]\n",
        "                proba[mask] = tmp\n",
        "        return proba\n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.predict_proba(X), axis=1)\n",
        "    def _get_features_on_path(self, leaf_id):\n",
        "        tree = self.tree_.tree_\n",
        "        def recurse(node, used):\n",
        "            if tree.children_left[node] == -1 and tree.children_right[node] == -1:\n",
        "                return used if node == leaf_id else None\n",
        "            if tree.feature[node] >= 0:\n",
        "                left = tree.children_left[node]\n",
        "                right = tree.children_right[node]\n",
        "                if left != -1:\n",
        "                    r = recurse(left, used | {int(tree.feature[node])})\n",
        "                    if r is not None:\n",
        "                        return r\n",
        "                if right != -1:\n",
        "                    r = recurse(right, used | {int(tree.feature[node])})\n",
        "                    if r is not None:\n",
        "                        return r\n",
        "            return None\n",
        "        res = recurse(0, set())\n",
        "        return res or set()\n"
      ],
      "metadata": {
        "id": "v-HBLsnSb5ta"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "feature_names = wine.feature_names\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n"
      ],
      "metadata": {
        "id": "asOR0jc7FKOq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = tune_lmt_with_optuna(X_train, y_train, n_trials=25)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dpV74VfcAVS",
        "outputId": "f38c79b1-2787-4ea1-f3ab-18251cc19a44"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-09-19 13:32:28,021] A new study created in memory with name: no-name-6efa911f-9580-4e5c-b904-cca6302d92b9\n",
            "[I 2025-09-19 13:32:28,400] Trial 0 finished with value: 0.9185826078439889 and parameters: {'max_depth': 4, 'min_samples_leaf': 191, 'reuse_ratio': 0.585595153449124, 'topk_frac': 0.6789267873576292, 'C': 0.004207988669606638, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9185826078439889.\n",
            "[I 2025-09-19 13:32:28,869] Trial 1 finished with value: 0.9017217818988715 and parameters: {'max_depth': 7, 'min_samples_leaf': 124, 'reuse_ratio': 0.5664580622368364, 'topk_frac': 0.21646759543664196, 'C': 7.579479953348009, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.9185826078439889.\n",
            "[I 2025-09-19 13:32:29,355] Trial 2 finished with value: 0.9343255676555987 and parameters: {'max_depth': 3, 'min_samples_leaf': 45, 'reuse_ratio': 0.2433937943676302, 'topk_frac': 0.6198051453057902, 'C': 0.05342937261279776, 'solver': 'saga'}. Best is trial 2 with value: 0.9343255676555987.\n",
            "[I 2025-09-19 13:32:29,743] Trial 3 finished with value: 0.976 and parameters: {'max_depth': 2, 'min_samples_leaf': 65, 'reuse_ratio': 0.2930894746349534, 'topk_frac': 0.5648559873736287, 'C': 1.382623217936987, 'solver': 'saga'}. Best is trial 3 with value: 0.976.\n",
            "[I 2025-09-19 13:32:30,199] Trial 4 finished with value: 0.865212960826583 and parameters: {'max_depth': 5, 'min_samples_leaf': 18, 'reuse_ratio': 0.48603588152115074, 'topk_frac': 0.33641929894983325, 'C': 0.0018205657658407262, 'solver': 'saga'}. Best is trial 3 with value: 0.976.\n",
            "[I 2025-09-19 13:32:30,447] Trial 5 finished with value: 0.9760971659919029 and parameters: {'max_depth': 6, 'min_samples_leaf': 68, 'reuse_ratio': 0.07813769120510711, 'topk_frac': 0.7473864212097256, 'C': 0.057624872164786026, 'solver': 'saga'}. Best is trial 5 with value: 0.9760971659919029.\n",
            "[I 2025-09-19 13:32:30,666] Trial 6 finished with value: 0.9755873015873016 and parameters: {'max_depth': 2, 'min_samples_leaf': 183, 'reuse_ratio': 0.20702398528001353, 'topk_frac': 0.7300178274831857, 'C': 0.017654048052495083, 'solver': 'saga'}. Best is trial 5 with value: 0.9760971659919029.\n",
            "[I 2025-09-19 13:32:30,927] Trial 7 finished with value: 0.98415330634278 and parameters: {'max_depth': 3, 'min_samples_leaf': 195, 'reuse_ratio': 0.6201062586888917, 'topk_frac': 0.9515991532513512, 'C': 3.7958531426706403, 'solver': 'saga'}. Best is trial 7 with value: 0.98415330634278.\n",
            "[I 2025-09-19 13:32:31,250] Trial 8 finished with value: 0.9179022805883488 and parameters: {'max_depth': 2, 'min_samples_leaf': 47, 'reuse_ratio': 0.03618183112843045, 'topk_frac': 0.4602642646106115, 'C': 0.03586816498627549, 'solver': 'saga'}. Best is trial 7 with value: 0.98415330634278.\n",
            "[I 2025-09-19 13:32:31,473] Trial 9 finished with value: 0.9591843137254902 and parameters: {'max_depth': 4, 'min_samples_leaf': 63, 'reuse_ratio': 0.4341568665265988, 'topk_frac': 0.31273937997981016, 'C': 1.6172900811143154, 'solver': 'saga'}. Best is trial 7 with value: 0.98415330634278.\n",
            "[I 2025-09-19 13:32:31,721] Trial 10 finished with value: 0.98415330634278 and parameters: {'max_depth': 5, 'min_samples_leaf': 143, 'reuse_ratio': 0.7756296886302927, 'topk_frac': 0.9641119495811593, 'C': 0.4050104259141603, 'solver': 'lbfgs'}. Best is trial 7 with value: 0.98415330634278.\n",
            "[I 2025-09-19 13:32:31,956] Trial 11 finished with value: 0.98415330634278 and parameters: {'max_depth': 5, 'min_samples_leaf': 155, 'reuse_ratio': 0.771094370186789, 'topk_frac': 0.9979547888017002, 'C': 0.5325924546877745, 'solver': 'lbfgs'}. Best is trial 7 with value: 0.98415330634278.\n",
            "[I 2025-09-19 13:32:32,215] Trial 12 finished with value: 0.98415330634278 and parameters: {'max_depth': 3, 'min_samples_leaf': 143, 'reuse_ratio': 0.7939510750285534, 'topk_frac': 0.9730318397029099, 'C': 0.334155174571091, 'solver': 'lbfgs'}. Best is trial 7 with value: 0.98415330634278.\n",
            "[I 2025-09-19 13:32:32,460] Trial 13 finished with value: 0.9760971659919029 and parameters: {'max_depth': 6, 'min_samples_leaf': 108, 'reuse_ratio': 0.6635318538413146, 'topk_frac': 0.868461466831221, 'C': 8.882580190235517, 'solver': 'lbfgs'}. Best is trial 7 with value: 0.98415330634278.\n",
            "[I 2025-09-19 13:32:32,706] Trial 14 finished with value: 0.984 and parameters: {'max_depth': 3, 'min_samples_leaf': 166, 'reuse_ratio': 0.67720396377515, 'topk_frac': 0.8696333901053103, 'C': 0.24179786496663208, 'solver': 'lbfgs'}. Best is trial 7 with value: 0.98415330634278.\n",
            "[I 2025-09-19 13:32:32,948] Trial 15 finished with value: 0.9760971659919029 and parameters: {'max_depth': 4, 'min_samples_leaf': 133, 'reuse_ratio': 0.6871040028302894, 'topk_frac': 0.8617880710728635, 'C': 2.376404722989419, 'solver': 'lbfgs'}. Best is trial 7 with value: 0.98415330634278.\n",
            "[I 2025-09-19 13:32:33,211] Trial 16 finished with value: 0.9593217961948612 and parameters: {'max_depth': 6, 'min_samples_leaf': 173, 'reuse_ratio': 0.5581147562301265, 'topk_frac': 0.8121482541857805, 'C': 3.5376644422402417, 'solver': 'lbfgs'}. Best is trial 7 with value: 0.98415330634278.\n",
            "[I 2025-09-19 13:32:33,447] Trial 17 finished with value: 0.9756854700854699 and parameters: {'max_depth': 5, 'min_samples_leaf': 198, 'reuse_ratio': 0.3618610503020867, 'topk_frac': 0.9391708485021248, 'C': 0.20809622306598385, 'solver': 'saga'}. Best is trial 7 with value: 0.98415330634278.\n",
            "[I 2025-09-19 13:32:33,688] Trial 18 finished with value: 0.976 and parameters: {'max_depth': 7, 'min_samples_leaf': 95, 'reuse_ratio': 0.7445667486142439, 'topk_frac': 0.5483345311436363, 'C': 0.9089297613461134, 'solver': 'saga'}. Best is trial 7 with value: 0.98415330634278.\n",
            "[I 2025-09-19 13:32:33,925] Trial 19 finished with value: 0.9676293297345928 and parameters: {'max_depth': 3, 'min_samples_leaf': 152, 'reuse_ratio': 0.6200634613092506, 'topk_frac': 0.7687149523534026, 'C': 0.14776193110470198, 'solver': 'lbfgs'}. Best is trial 7 with value: 0.98415330634278.\n",
            "[I 2025-09-19 13:32:34,215] Trial 20 finished with value: 0.9760971659919029 and parameters: {'max_depth': 4, 'min_samples_leaf': 116, 'reuse_ratio': 0.4961191262276459, 'topk_frac': 0.9153294678903858, 'C': 3.7141328058364405, 'solver': 'saga'}. Best is trial 7 with value: 0.98415330634278.\n",
            "[I 2025-09-19 13:32:34,445] Trial 21 finished with value: 0.98415330634278 and parameters: {'max_depth': 5, 'min_samples_leaf': 165, 'reuse_ratio': 0.7993593783268043, 'topk_frac': 0.9813797043499264, 'C': 0.47571890554293267, 'solver': 'lbfgs'}. Best is trial 7 with value: 0.98415330634278.\n",
            "[I 2025-09-19 13:32:34,681] Trial 22 finished with value: 0.98415330634278 and parameters: {'max_depth': 5, 'min_samples_leaf': 147, 'reuse_ratio': 0.7229080020267236, 'topk_frac': 0.9810178400034624, 'C': 0.6107318967144559, 'solver': 'lbfgs'}. Best is trial 7 with value: 0.98415330634278.\n",
            "[I 2025-09-19 13:32:34,933] Trial 23 finished with value: 0.98415330634278 and parameters: {'max_depth': 6, 'min_samples_leaf': 178, 'reuse_ratio': 0.7380454442404406, 'topk_frac': 0.9976628688835076, 'C': 0.7851359960260249, 'solver': 'lbfgs'}. Best is trial 7 with value: 0.98415330634278.\n",
            "[I 2025-09-19 13:32:35,191] Trial 24 finished with value: 0.9760971659919029 and parameters: {'max_depth': 5, 'min_samples_leaf': 157, 'reuse_ratio': 0.6460400705392448, 'topk_frac': 0.9022884239910017, 'C': 5.560118171827009, 'solver': 'lbfgs'}. Best is trial 7 with value: 0.98415330634278.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# дообучаем LMT на лучших параметрах\n",
        "lmt_best = LogisticModelTree(**{**best_params, \"max_iter\": 5000, \"random_state\": 42})\n",
        "lmt_best.fit(X_train, y_train)\n",
        "y_pred_lmt = lmt_best.predict(X_test)\n",
        "def metrics_row(name, y_true, y_pred):\n",
        "    return {\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"Precision\": precision_score(y_true, y_pred, average=\"weighted\"),\n",
        "        \"Recall\": recall_score(y_true, y_pred, average=\"weighted\"),\n",
        "        \"F1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
        "        \"F2\": fbeta_score(y_true, y_pred, beta=2, average=\"weighted\")\n",
        "    }\n",
        "rows = [metrics_row(\"LMT (Optuna)\", y_test, y_pred_lmt)]\n",
        "# Logistic Regression (со скейлингом)\n",
        "base_lr = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    LogisticRegression(max_iter=5000, solver=\"lbfgs\", multi_class=\"auto\")\n",
        ").fit(X_train, y_train)\n",
        "rows.append(metrics_row(\"Logistic Regression\", y_test, base_lr.predict(X_test)))\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=400, random_state=42, n_jobs=-1\n",
        ").fit(X_train, y_train)\n",
        "rows.append(metrics_row(\"Random Forest\", y_test, rf.predict(X_test)))\n",
        "# XGBoost\n",
        "try:\n",
        "    xgb = XGBClassifier(\n",
        "        objective=\"multi:softprob\",\n",
        "        num_class=len(np.unique(y_train)),\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=5,\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.9,\n",
        "        reg_lambda=1.0,\n",
        "        eval_metric=\"mlogloss\",\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ).fit(X_train, y_train)\n",
        "    rows.append(metrics_row(\"XGBoost\", y_test, xgb.predict(X_test)))\n",
        "except ImportError:\n",
        "    print(\"XGBoost недоступен\")\n",
        "results = pd.DataFrame(rows).sort_values(\"Accuracy\", ascending=False)\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpW4y1LweCGb",
        "outputId": "d2834897-04e7-4bd5-d766-13bed1d6e0b9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning:\n",
            "\n",
            "'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning:\n",
            "\n",
            "'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Model  Accuracy  Precision    Recall        F1        F2\n",
            "3              XGBoost  1.000000   1.000000  1.000000  1.000000  1.000000\n",
            "2        Random Forest  1.000000   1.000000  1.000000  1.000000  1.000000\n",
            "1  Logistic Regression  0.981481   0.982456  0.981481  0.981506  0.981380\n",
            "0         LMT (Optuna)  0.962963   0.963938  0.962963  0.962894  0.962803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost и RandomForest идеально решают Wine,\n",
        "Logistic Regression на скейлинге показывает очень достойно: почти 98%.\n",
        "LMT (Optuna) отстаёт (96%), но всё равно выше, чем ожидалось для интерпретируемой гибридной модели.\n",
        "Выводы\n",
        "Wine — относительно простой датасет. Ансамбли деревьев справляются идеально.\n",
        "Логрег и LMT немного ошибаются, но дают хорошую интерпретируемость.\n",
        "\n",
        "Попробуем LMT бустинг и беггинг на датасете breast_cancer"
      ],
      "metadata": {
        "id": "A_gqrkw_FkPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# данные\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "def metrics_row(name, y_true, y_pred):\n",
        "    return {\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"Precision\": precision_score(y_true, y_pred, average=\"weighted\"),\n",
        "        \"Recall\": recall_score(y_true, y_pred, average=\"weighted\"),\n",
        "        \"F1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
        "        \"F2\": fbeta_score(y_true, y_pred, beta=2, average=\"weighted\")\n",
        "    }\n",
        "rows = []\n",
        "# --- базовый LMT  ---\n",
        "base_lmt = LogisticModelTree(\n",
        "    max_depth=3, min_samples_leaf=30, random_state=42,\n",
        "    reuse_ratio=0.2, topk_frac=1.0, C=1.0, solver=\"lbfgs\", max_iter=5000\n",
        ")\n",
        "base_lmt.fit(X_train, y_train)\n",
        "rows.append(metrics_row(\"LMT (base)\", y_test, base_lmt.predict(X_test)))\n",
        "# --- LMT-Bagging ---\n",
        "lmt_for_bag = LogisticModelTree(\n",
        "    max_depth=3, min_samples_leaf=25, random_state=42,\n",
        "    reuse_ratio=0.2, topk_frac=0.8, C=1.0, solver=\"lbfgs\", max_iter=5000\n",
        ")\n",
        "bag = BaggingClassifier(\n",
        "    estimator=lmt_for_bag,\n",
        "    n_estimators=25,\n",
        "    max_samples=0.8,\n",
        "    max_features=1.0,\n",
        "    bootstrap=True,\n",
        "    bootstrap_features=False,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "rows.append(metrics_row(\"LMT-Bagging (25x, 80%)\", y_test, bag.predict(X_test)))\n",
        "# --- LMT-Boosting (AdaBoost; \"SAMME\" теперь универсальный) ---\n",
        "\n",
        "class LogisticModelTree(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, max_depth=3, min_samples_leaf=20, random_state=None,\n",
        "                 reuse_ratio=0.1, topk_frac=1.0, C=1.0, solver=\"lbfgs\", max_iter=5000):\n",
        "        self.max_depth=max_depth; self.min_samples_leaf=min_samples_leaf; self.random_state=random_state\n",
        "        self.reuse_ratio=reuse_ratio; self.topk_frac=topk_frac; self.C=C; self.solver=solver; self.max_iter=max_iter\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None):\n",
        "        self.tree_ = DecisionTreeClassifier(max_depth=self.max_depth,\n",
        "                                            min_samples_leaf=self.min_samples_leaf,\n",
        "                                            random_state=self.random_state)\n",
        "        self.tree_.fit(X, y, sample_weight=sample_weight)\n",
        "\n",
        "        leaf_ids = self.tree_.apply(X)\n",
        "        self.classes_ = np.array(self.tree_.classes_)\n",
        "        self.class_to_index_ = {c:i for i,c in enumerate(self.classes_)}\n",
        "        self.models_ = {}\n",
        "        rng = np.random.RandomState(self.random_state)\n",
        "        n_features = X.shape[1]\n",
        "        sw = sample_weight if sample_weight is not None else np.ones(len(y), float)\n",
        "\n",
        "        for leaf in np.unique(leaf_ids):\n",
        "            mask = (leaf_ids == leaf)\n",
        "            y_leaf = y[mask]; X_leaf_full = X[mask]; sw_leaf = sw[mask]\n",
        "\n",
        "            # признаки: неиспользованные + часть \"путевых\"\n",
        "            path = self._get_features_on_path(leaf)\n",
        "            unused = [i for i in range(n_features) if i not in path]\n",
        "            k_reuse = int(len(path)*self.reuse_ratio) if path else 0\n",
        "            reuse = rng.choice(list(path), size=k_reuse, replace=False).tolist() if k_reuse>0 else []\n",
        "            final_feats = unused + reuse or list(range(n_features))\n",
        "\n",
        "            uniq = np.unique(y_leaf)\n",
        "            if len(uniq)==1:\n",
        "                cls_idx = self.class_to_index_[uniq[0]]\n",
        "                def dummy(X_in, c=cls_idx, n=len(self.classes_)):\n",
        "                    P = np.zeros((X_in.shape[0], n)); P[:,c]=1.0; return P\n",
        "                self.models_[leaf] = {\"is_dummy\":True, \"feats\":final_feats, \"model\":dummy, \"leaf_classes\":np.array([uniq[0]])}\n",
        "                continue\n",
        "\n",
        "            X_sub = X_leaf_full[:, final_feats]\n",
        "            if X_sub.shape[0] >= 5:\n",
        "                mi = mutual_info_classif(X_sub, y_leaf, random_state=self.random_state)\n",
        "                order = np.argsort(mi)[::-1]\n",
        "                k_top = max(1, int(ceil(len(final_feats)*self.topk_frac)))\n",
        "                keep = order[:k_top]\n",
        "                feats = [final_feats[i] for i in keep]\n",
        "            else:\n",
        "                feats = final_feats\n",
        "\n",
        "            scaler = StandardScaler().fit(X_leaf_full[:, feats])\n",
        "            Xs = scaler.transform(X_leaf_full[:, feats])\n",
        "            lr = LogisticRegression(max_iter=self.max_iter, solver=self.solver, C=self.C)\n",
        "            lr.fit(Xs, y_leaf, sample_weight=sw_leaf)\n",
        "\n",
        "            self.models_[leaf] = {\"is_dummy\":False, \"feats\":feats,\n",
        "                                  \"model\":(scaler, lr), \"leaf_classes\":np.array(lr.classes_)}\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        leaf_ids = self.tree_.apply(X)\n",
        "        proba = np.zeros((X.shape[0], len(self.classes_)))\n",
        "        for leaf, blob in self.models_.items():\n",
        "            mask = (leaf_ids == leaf)\n",
        "            if not np.any(mask): continue\n",
        "            feats = blob[\"feats\"]; X_leaf = X[mask][:, feats]\n",
        "            if blob[\"is_dummy\"]:\n",
        "                proba[mask] = blob[\"model\"](X_leaf)\n",
        "            else:\n",
        "                scaler, lr = blob[\"model\"]\n",
        "                local = lr.predict_proba(scaler.transform(X_leaf))\n",
        "                tmp = np.zeros((local.shape[0], len(self.classes_)))\n",
        "                for j, cls in enumerate(blob[\"leaf_classes\"]):\n",
        "                    tmp[:, self.class_to_index_[cls]] = local[:, j]\n",
        "                proba[mask] = tmp\n",
        "        return proba\n",
        "\n",
        "    def predict(self, X): return np.argmax(self.predict_proba(X), axis=1)\n",
        "\n",
        "    def _get_features_on_path(self, leaf_id):\n",
        "        t = self.tree_.tree_\n",
        "        def rec(node, used):\n",
        "            if t.children_left[node]==-1 and t.children_right[node]==-1:\n",
        "                return used if node==leaf_id else None\n",
        "            if t.feature[node] >= 0:\n",
        "                left, right = t.children_left[node], t.children_right[node]\n",
        "                r = rec(left, used|{int(t.feature[node])});  r = r if r is not None else rec(right, used|{int(t.feature[node])})\n",
        "                return r\n",
        "            return None\n",
        "        return rec(0, set()) or set()\n",
        "\n",
        "boost = AdaBoostClassifier(\n",
        "    estimator=LogisticModelTree(max_depth=2, min_samples_leaf=20, random_state=42,\n",
        "                                reuse_ratio=0.3, topk_frac=0.9, C=1.0, solver=\"lbfgs\", max_iter=5000),\n",
        "    n_estimators=30, learning_rate=0.5, random_state=42\n",
        ")\n",
        "\n",
        "boost.fit(X_train, y_train)\n",
        "rows.append(metrics_row(\"LMT-Boosting (AdaBoost, 30, 0.5)\", y_test, boost.predict(X_test)))\n",
        "# --- Бейзлайны: LogisticRegression / RandomForest / XGBoost ---\n",
        "lr = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    LogisticRegression(max_iter=5000, solver=\"lbfgs\")\n",
        ").fit(X_train, y_train)\n",
        "rows.append(metrics_row(\"Logistic Regression\", y_test, lr.predict(X_test)))\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=400, random_state=42, n_jobs=-1\n",
        ").fit(X_train, y_train)\n",
        "rows.append(metrics_row(\"Random Forest\", y_test, rf.predict(X_test)))\n",
        "try:\n",
        "    xgb = XGBClassifier(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=4,\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.9,\n",
        "        reg_lambda=1.0,\n",
        "        eval_metric=\"logloss\",\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ).fit(X_train, y_train)\n",
        "    rows.append(metrics_row(\"XGBoost\", y_test, xgb.predict(X_test)))\n",
        "except Exception as e:\n",
        "    pass\n",
        "results = pd.DataFrame(rows).sort_values(\"Accuracy\", ascending=False)\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw4AKOREfpaq",
        "outputId": "e5e6f65c-f505-4d4b-b189-93cd51c9bae1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning:\n",
            "\n",
            "'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning:\n",
            "\n",
            "'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning:\n",
            "\n",
            "'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning:\n",
            "\n",
            "'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                              Model  Accuracy  Precision    Recall        F1  \\\n",
            "3               Logistic Regression  0.988304   0.988304  0.988304  0.988304   \n",
            "2  LMT-Boosting (AdaBoost, 30, 0.5)  0.964912   0.964964  0.964912  0.964796   \n",
            "5                           XGBoost  0.964912   0.965576  0.964912  0.964668   \n",
            "0                        LMT (base)  0.953216   0.953216  0.953216  0.953216   \n",
            "1            LMT-Bagging (25x, 80%)  0.947368   0.947463  0.947368  0.947101   \n",
            "4                     Random Forest  0.947368   0.947463  0.947368  0.947101   \n",
            "\n",
            "         F2  \n",
            "3  0.988304  \n",
            "2  0.964832  \n",
            "5  0.964679  \n",
            "0  0.953216  \n",
            "1  0.947187  \n",
            "4  0.947187  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Логистическая регрессия — победитель. Датасет почти линейно разделим, поэтому ансамбли и гибриды даже проигрывают по метрикам.\n",
        "\n",
        "LMT-Boosting ≈ XGBoost —  бустинг на LMT вышел на уровень XGBoost, хотя тот куда более оптимизирован.\n",
        "Bagging улучшает базовый LMT, но не так сильно, как бустинг.\n",
        "Random Forest — хуже всех, что тоже ожидаемо: дерево «дробит» пространство, а Breast Cancer этому не очень подходит.\n",
        "\n",
        "Вывод\n",
        "На «чистых» и почти линейных данных глобальная логрег остаётся топом.\n",
        "LMT+Boosting показал, что может соревноваться с XGBoost. На более сложных данных он может раскрыться ещё лучше.\n",
        "Bagging стабилизирует, но не даёт драматического прироста."
      ],
      "metadata": {
        "id": "OG0Rka6cG9_x"
      }
    }
  ]
}